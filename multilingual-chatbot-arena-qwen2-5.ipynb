{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c08c34",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-10T09:13:07.576671Z",
     "iopub.status.busy": "2025-01-10T09:13:07.576310Z",
     "iopub.status.idle": "2025-01-10T09:13:08.245211Z",
     "shell.execute_reply": "2025-01-10T09:13:08.244464Z"
    },
    "papermill": {
     "duration": 0.676057,
     "end_time": "2025-01-10T09:13:08.246830",
     "exception": false,
     "start_time": "2025-01-10T09:13:07.570773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67b1786",
   "metadata": {
    "papermill": {
     "duration": 0.003237,
     "end_time": "2025-01-10T09:13:08.253987",
     "exception": false,
     "start_time": "2025-01-10T09:13:08.250750",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20e9b5b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T09:13:08.261710Z",
     "iopub.status.busy": "2025-01-10T09:13:08.261270Z",
     "iopub.status.idle": "2025-01-10T09:13:13.003623Z",
     "shell.execute_reply": "2025-01-10T09:13:13.002617Z"
    },
    "papermill": {
     "duration": 4.748069,
     "end_time": "2025-01-10T09:13:13.005367",
     "exception": false,
     "start_time": "2025-01-10T09:13:08.257298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install peft \\\n",
    "    -U --no-index --find-links /kaggle/input/lmsys-wheel-files --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c7b59c",
   "metadata": {
    "papermill": {
     "duration": 0.003474,
     "end_time": "2025-01-10T09:13:13.012923",
     "exception": false,
     "start_time": "2025-01-10T09:13:13.009449",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9f798c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T09:13:13.020905Z",
     "iopub.status.busy": "2025-01-10T09:13:13.020571Z",
     "iopub.status.idle": "2025-01-10T09:13:19.184687Z",
     "shell.execute_reply": "2025-01-10T09:13:19.183740Z"
    },
    "papermill": {
     "duration": 6.169861,
     "end_time": "2025-01-10T09:13:19.186374",
     "exception": false,
     "start_time": "2025-01-10T09:13:13.016513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import polars as pl\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6569b082",
   "metadata": {
    "papermill": {
     "duration": 0.003109,
     "end_time": "2025-01-10T09:13:19.193363",
     "exception": false,
     "start_time": "2025-01-10T09:13:19.190254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3cd14e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T09:13:19.201349Z",
     "iopub.status.busy": "2025-01-10T09:13:19.200897Z",
     "iopub.status.idle": "2025-01-10T09:13:19.205382Z",
     "shell.execute_reply": "2025-01-10T09:13:19.204549Z"
    },
    "papermill": {
     "duration": 0.009976,
     "end_time": "2025-01-10T09:13:19.206689",
     "exception": false,
     "start_time": "2025-01-10T09:13:19.196713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    qwen_dir='/kaggle/input/qwen2.5/transformers/1.5b-instruct/1'\n",
    "    max_length=2048\n",
    "    device=torch.device('cuda')\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8df0eb",
   "metadata": {
    "papermill": {
     "duration": 0.003078,
     "end_time": "2025-01-10T09:13:19.213061",
     "exception": false,
     "start_time": "2025-01-10T09:13:19.209983",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36b4d1fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T09:13:19.220439Z",
     "iopub.status.busy": "2025-01-10T09:13:19.220192Z",
     "iopub.status.idle": "2025-01-10T09:13:19.223302Z",
     "shell.execute_reply": "2025-01-10T09:13:19.222705Z"
    },
    "papermill": {
     "duration": 0.008214,
     "end_time": "2025-01-10T09:13:19.224494",
     "exception": false,
     "start_time": "2025-01-10T09:13:19.216280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    return pl.read_parquet(file_path).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a17c2",
   "metadata": {
    "papermill": {
     "duration": 0.003069,
     "end_time": "2025-01-10T09:13:19.230866",
     "exception": false,
     "start_time": "2025-01-10T09:13:19.227797",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33e1ac9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T09:13:19.239546Z",
     "iopub.status.busy": "2025-01-10T09:13:19.239267Z",
     "iopub.status.idle": "2025-01-10T09:13:19.243018Z",
     "shell.execute_reply": "2025-01-10T09:13:19.242329Z"
    },
    "papermill": {
     "duration": 0.00898,
     "end_time": "2025-01-10T09:13:19.244204",
     "exception": false,
     "start_time": "2025-01-10T09:13:19.235224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    df.prompt = df.prompt.fillna('')\n",
    "    df.response_a = df.response_a.fillna('')\n",
    "    df.response_b = df.response_b.fillna('')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0413f0d7",
   "metadata": {
    "papermill": {
     "duration": 0.00298,
     "end_time": "2025-01-10T09:13:19.250361",
     "exception": false,
     "start_time": "2025-01-10T09:13:19.247381",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Initialize Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8552ec33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T09:13:19.257679Z",
     "iopub.status.busy": "2025-01-10T09:13:19.257397Z",
     "iopub.status.idle": "2025-01-10T09:13:35.330566Z",
     "shell.execute_reply": "2025-01-10T09:13:35.329758Z"
    },
    "papermill": {
     "duration": 16.078369,
     "end_time": "2025-01-10T09:13:35.331901",
     "exception": false,
     "start_time": "2025-01-10T09:13:19.253532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.qwen_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(cfg.qwen_dir)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a291c98e",
   "metadata": {
    "papermill": {
     "duration": 0.003288,
     "end_time": "2025-01-10T09:13:35.339060",
     "exception": false,
     "start_time": "2025-01-10T09:13:35.335772",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tokenise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d9b6f10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T09:13:35.346778Z",
     "iopub.status.busy": "2025-01-10T09:13:35.346477Z",
     "iopub.status.idle": "2025-01-10T09:13:35.351939Z",
     "shell.execute_reply": "2025-01-10T09:13:35.351063Z"
    },
    "papermill": {
     "duration": 0.010826,
     "end_time": "2025-01-10T09:13:35.353240",
     "exception": false,
     "start_time": "2025-01-10T09:13:35.342414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "instruction = \"\"\"In the text provided for you below, PROMPT is the question presented; MODEL_A is the response from the first model; MODEL_B is the response from the second model. Please select the best answer from the two responses provided. If the first answer is better, return \"model_a\"; if the second answer is better, return \"model_b\".\"\"\"\n",
    "\n",
    "def tokenize_data(df):\n",
    "    tokenised_data = []\n",
    "    ids = []\n",
    "\n",
    "    for idx in range(len(df)):\n",
    "\n",
    "        rec = df.iloc[idx,:]\n",
    "\n",
    "        prompt = 'PROMPT: ' + rec['prompt']\n",
    "        model_a = 'MODEL_A: ' + rec['response_a']\n",
    "        model_b = 'MODEL_B: ' + rec['response_b']\n",
    "        text = prompt + model_a + model_b\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    \n",
    "        tokenised_datum = tokenizer(\n",
    "            [text],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        ).to(cfg.device)\n",
    "\n",
    "        tokenised_data.append(tokenised_datum)\n",
    "        ids.append(rec['id'])\n",
    "\n",
    "    return tokenised_data, ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ee2a38",
   "metadata": {
    "papermill": {
     "duration": 0.003231,
     "end_time": "2025-01-10T09:13:35.359901",
     "exception": false,
     "start_time": "2025-01-10T09:13:35.356670",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Method to Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0cef859",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T09:13:35.367652Z",
     "iopub.status.busy": "2025-01-10T09:13:35.367352Z",
     "iopub.status.idle": "2025-01-10T09:13:35.371550Z",
     "shell.execute_reply": "2025-01-10T09:13:35.370770Z"
    },
    "papermill": {
     "duration": 0.009567,
     "end_time": "2025-01-10T09:13:35.372975",
     "exception": false,
     "start_time": "2025-01-10T09:13:35.363408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(tokenised_datum):\n",
    "    model.to(cfg.device)\n",
    "    input_ids = tokenised_datum.input_ids\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=5\n",
    "    )\n",
    "\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ea2a6",
   "metadata": {
    "papermill": {
     "duration": 0.003118,
     "end_time": "2025-01-10T09:13:35.379475",
     "exception": false,
     "start_time": "2025-01-10T09:13:35.376357",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15a54530",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T09:13:35.387115Z",
     "iopub.status.busy": "2025-01-10T09:13:35.386838Z",
     "iopub.status.idle": "2025-01-10T14:52:11.941341Z",
     "shell.execute_reply": "2025-01-10T14:52:11.940272Z"
    },
    "papermill": {
     "duration": 20316.560635,
     "end_time": "2025-01-10T14:52:11.943473",
     "exception": false,
     "start_time": "2025-01-10T09:13:35.382838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: CUDA out of memory. Tried to allocate 9.18 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.37 GiB is free. Process 2695 has 8.52 GiB memory in use. Of the allocated memory 8.15 GiB is allocated by PyTorch, and 86.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 11.55 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.09 GiB is free. Process 2695 has 8.79 GiB memory in use. Of the allocated memory 8.39 GiB is allocated by PyTorch, and 112.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 7.60 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.52 GiB is free. Process 2695 has 8.36 GiB memory in use. Of the allocated memory 7.98 GiB is allocated by PyTorch, and 95.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 7.84 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.50 GiB is free. Process 2695 has 8.38 GiB memory in use. Of the allocated memory 8.01 GiB is allocated by PyTorch, and 90.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 8.65 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.42 GiB is free. Process 2695 has 8.46 GiB memory in use. Of the allocated memory 8.09 GiB is allocated by PyTorch, and 83.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 7.76 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.50 GiB is free. Process 2695 has 8.38 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 98.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 8.95 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.39 GiB is free. Process 2695 has 8.50 GiB memory in use. Of the allocated memory 8.12 GiB is allocated by PyTorch, and 90.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 9.82 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.27 GiB is free. Process 2695 has 8.62 GiB memory in use. Of the allocated memory 8.21 GiB is allocated by PyTorch, and 118.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 8.21 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.44 GiB is free. Process 2695 has 8.44 GiB memory in use. Of the allocated memory 8.04 GiB is allocated by PyTorch, and 110.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 7.86 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.48 GiB is free. Process 2695 has 8.40 GiB memory in use. Of the allocated memory 8.01 GiB is allocated by PyTorch, and 107.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 7.83 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.50 GiB is free. Process 2695 has 8.38 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 91.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 9.32 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.35 GiB is free. Process 2695 has 8.54 GiB memory in use. Of the allocated memory 8.16 GiB is allocated by PyTorch, and 91.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 8.78 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.39 GiB is free. Process 2695 has 8.50 GiB memory in use. Of the allocated memory 8.10 GiB is allocated by PyTorch, and 109.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 7.87 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.48 GiB is free. Process 2695 has 8.40 GiB memory in use. Of the allocated memory 8.01 GiB is allocated by PyTorch, and 106.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 7.85 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.50 GiB is free. Process 2695 has 8.38 GiB memory in use. Of the allocated memory 8.01 GiB is allocated by PyTorch, and 89.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 12.53 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.00 GiB is free. Process 2695 has 8.89 GiB memory in use. Of the allocated memory 8.50 GiB is allocated by PyTorch, and 108.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 8.69 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.42 GiB is free. Process 2695 has 8.46 GiB memory in use. Of the allocated memory 8.10 GiB is allocated by PyTorch, and 78.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 11.37 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.11 GiB is free. Process 2695 has 8.77 GiB memory in use. Of the allocated memory 8.37 GiB is allocated by PyTorch, and 112.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 8.34 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.42 GiB is free. Process 2695 has 8.46 GiB memory in use. Of the allocated memory 8.06 GiB is allocated by PyTorch, and 116.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 8.68 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.41 GiB is free. Process 2695 has 8.48 GiB memory in use. Of the allocated memory 8.09 GiB is allocated by PyTorch, and 99.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 8.53 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.42 GiB is free. Process 2695 has 8.46 GiB memory in use. Of the allocated memory 8.08 GiB is allocated by PyTorch, and 96.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 7.91 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.50 GiB is free. Process 2695 has 8.38 GiB memory in use. Of the allocated memory 8.01 GiB is allocated by PyTorch, and 82.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 10.51 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.23 GiB is free. Process 2695 has 8.66 GiB memory in use. Of the allocated memory 8.29 GiB is allocated by PyTorch, and 84.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 8.48 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.41 GiB is free. Process 2695 has 8.48 GiB memory in use. Of the allocated memory 8.07 GiB is allocated by PyTorch, and 121.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 9.94 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.27 GiB is free. Process 2695 has 8.62 GiB memory in use. Of the allocated memory 8.23 GiB is allocated by PyTorch, and 105.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 9.62 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.31 GiB is free. Process 2695 has 8.58 GiB memory in use. Of the allocated memory 8.19 GiB is allocated by PyTorch, and 99.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 7.80 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.50 GiB is free. Process 2695 has 8.38 GiB memory in use. Of the allocated memory 8.00 GiB is allocated by PyTorch, and 93.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 9.91 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.27 GiB is free. Process 2695 has 8.62 GiB memory in use. Of the allocated memory 8.22 GiB is allocated by PyTorch, and 107.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "An error occurred: CUDA out of memory. Tried to allocate 7.70 GiB. GPU 0 has a total capacity of 15.89 GiB of which 7.52 GiB is free. Process 2695 has 8.36 GiB memory in use. Of the allocated memory 7.99 GiB is allocated by PyTorch, and 85.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "train = load_data('/kaggle/input/wsdm-cup-multilingual-chatbot-arena/train.parquet')\n",
    "train = train.head(25000)\n",
    "train = preprocess_data(train)\n",
    "tokenised_data_train, ids = tokenize_data(train)\n",
    "predictions_train = []\n",
    "\n",
    "for tokenised_datum_train in tokenised_data_train:\n",
    "    try:\n",
    "        torch.cuda.empty_cache()  # Clears GPU memory\n",
    "        response = inference(tokenised_datum_train)\n",
    "        predictions_train.append(response)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        predictions_train.append(\"model_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918f8537",
   "metadata": {
    "papermill": {
     "duration": 0.00498,
     "end_time": "2025-01-10T14:52:12.015227",
     "exception": false,
     "start_time": "2025-01-10T14:52:12.010247",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0284dd11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T14:52:12.026334Z",
     "iopub.status.busy": "2025-01-10T14:52:12.025804Z",
     "iopub.status.idle": "2025-01-10T14:52:14.154799Z",
     "shell.execute_reply": "2025-01-10T14:52:14.154045Z"
    },
    "papermill": {
     "duration": 2.136295,
     "end_time": "2025-01-10T14:52:14.156408",
     "exception": false,
     "start_time": "2025-01-10T14:52:12.020113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = load_data('/kaggle/input/wsdm-cup-multilingual-chatbot-arena/test.parquet')\n",
    "test = preprocess_data(test)\n",
    "tokenised_data, ids = tokenize_data(test)\n",
    "predictions = []\n",
    "\n",
    "for tokenised_datum in tokenised_data:\n",
    "    try:\n",
    "        response = inference(tokenised_datum)\n",
    "        predictions.append(response)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        predictions.append(\"model_a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "594ff9a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-10T14:52:14.167801Z",
     "iopub.status.busy": "2025-01-10T14:52:14.167524Z",
     "iopub.status.idle": "2025-01-10T14:52:14.181947Z",
     "shell.execute_reply": "2025-01-10T14:52:14.181266Z"
    },
    "papermill": {
     "duration": 0.021205,
     "end_time": "2025-01-10T14:52:14.183160",
     "exception": false,
     "start_time": "2025-01-10T14:52:14.161955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>327228</td>\n",
       "      <td>model_a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1139415</td>\n",
       "      <td>model_a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1235630</td>\n",
       "      <td>model_a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   winner\n",
       "0   327228  model_a\n",
       "1  1139415  model_a\n",
       "2  1235630  model_a"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'winner': predictions\n",
    "})\n",
    "submission.to_csv(\"submission.csv\",index=False)\n",
    "submission"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10131489,
     "sourceId": 86946,
     "sourceType": "competition"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 141458,
     "sourceId": 166245,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20352.115523,
   "end_time": "2025-01-10T14:52:17.426892",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-10T09:13:05.311369",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
